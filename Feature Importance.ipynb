{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/ubuntu/spark-2.1.1-bin-hadoop2.7\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"yye090_722\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+\n",
      "|year|month|day|hour|WSPM|     station|PM25| SO2| NO2|   CO|  O3|TEMP|Season|PM_Rating|\n",
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+\n",
      "|2013|    3|  1|   0| 5.7|Nongzhanguan| 5.0| 4.0|12.0|200.0|85.0|-0.5|Spring|Excellent|\n",
      "|2013|    3|  1|   1| 3.9|Nongzhanguan| 8.0| 6.0|14.0|200.0|84.0|-0.7|Spring|Excellent|\n",
      "|2013|    3|  1|   2| 5.3|Nongzhanguan| 3.0| 5.0|14.0|200.0|83.0|-1.2|Spring|Excellent|\n",
      "|2013|    3|  1|   3| 4.9|Nongzhanguan| 5.0| 5.0|14.0|200.0|84.0|-1.4|Spring|Excellent|\n",
      "|2013|    3|  1|   4| 3.2|Nongzhanguan| 5.0| 6.0|21.0|200.0|77.0|-1.9|Spring|Excellent|\n",
      "|2013|    3|  1|   5| 2.4|Nongzhanguan| 3.0|13.0|21.0|300.0|77.0|-2.4|Spring|Excellent|\n",
      "|2013|    3|  1|   6| 2.2|Nongzhanguan| 4.0|15.0|32.0|300.0|62.0|-2.5|Spring|Excellent|\n",
      "|2013|    3|  1|   7| 3.0|Nongzhanguan| 3.0|14.0|45.0|400.0|48.0|-1.4|Spring|Excellent|\n",
      "|2013|    3|  1|   8| 4.6|Nongzhanguan| 3.0|13.0|49.0|400.0|46.0|-0.3|Spring|Excellent|\n",
      "|2013|    3|  1|   9| 5.5|Nongzhanguan|11.0| 9.0|28.0|400.0|68.0| 0.4|Spring|Excellent|\n",
      "|2013|    3|  1|  10| 5.2|Nongzhanguan| 3.0| 6.0|20.0|300.0|77.0| 1.4|Spring|Excellent|\n",
      "|2013|    3|  1|  11| 5.3|Nongzhanguan| 3.0| 6.0|19.0|300.0|81.0| 2.9|Spring|Excellent|\n",
      "|2013|    3|  1|  12| 4.6|Nongzhanguan| 3.0| 4.0|17.0|300.0|86.0| 4.0|Spring|Excellent|\n",
      "|2013|    3|  1|  13| 4.5|Nongzhanguan| 3.0| 4.0|17.0|300.0|87.0| 5.0|Spring|Excellent|\n",
      "|2013|    3|  1|  14| 4.0|Nongzhanguan| 6.0| 4.0|19.0|300.0|86.0| 6.2|Spring|Excellent|\n",
      "|2013|    3|  1|  15| 2.1|Nongzhanguan| 5.0| 4.0|18.0|300.0|89.0| 6.0|Spring|Excellent|\n",
      "|2013|    3|  1|  16| 1.8|Nongzhanguan| 5.0| 4.0|21.0|300.0|85.0| 5.6|Spring|Excellent|\n",
      "|2013|    3|  1|  17| 3.5|Nongzhanguan|10.0| 8.0|25.0|400.0|80.0| 4.4|Spring|Excellent|\n",
      "|2013|    3|  1|  18| 0.5|Nongzhanguan|11.0| 7.0|27.0|400.0|77.0| 3.2|Spring|Excellent|\n",
      "|2013|    3|  1|  19| 1.1|Nongzhanguan|11.0|12.0|39.0|400.0|64.0| 3.0|Spring|Excellent|\n",
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"./final1.csv\",format =\"csv\",header=\"true\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- WSPM: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- PM25: string (nullable = true)\n",
      " |-- SO2: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- CO: string (nullable = true)\n",
      " |-- O3: string (nullable = true)\n",
      " |-- TEMP: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- PM_Rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- WSPM: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- PM25: string (nullable = true)\n",
      " |-- SO2: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- CO: string (nullable = true)\n",
      " |-- O3: string (nullable = true)\n",
      " |-- TEMP: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- PM_Rating: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- WSPM: float (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- PM25: float (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      " |-- O3: float (nullable = true)\n",
      " |-- TEMP: float (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- PM_Rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "df.printSchema()\n",
    "changedTypedf = df.withColumn(\"year\", df[\"year\"].cast(\"integer\")).withColumn(\"month\", df[\"month\"].cast(\"integer\"))\\\n",
    ".withColumn(\"hour\", df[\"hour\"].cast(\"integer\"))\\\n",
    ".withColumn(\"day\", df[\"day\"].cast(\"integer\"))\\\n",
    ".withColumn(\"PM25\", df[\"PM25\"].cast(\"float\"))\\\n",
    ".withColumn(\"SO2\", df[\"SO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"NO2\", df[\"NO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"CO\", df[\"CO\"].cast(\"float\"))\\\n",
    ".withColumn(\"O3\", df[\"O3\"].cast(\"float\"))\\\n",
    ".withColumn(\"TEMP\", df[\"TEMP\"].cast(\"float\"))\\\n",
    ".withColumn(\"WSPM\", df[\"WSPM\"].cast(\"float\"))\n",
    "changedTypedf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+------------+-----------+--------------+\n",
      "|year|month|day|hour|WSPM|     station|PM25| SO2| NO2|   CO|  O3|TEMP|Season|PM_Rating|stationIndex|SeasonIndex|PM_RatingIndex|\n",
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+------------+-----------+--------------+\n",
      "|2013|    3|  1|   0| 5.7|Nongzhanguan| 5.0| 4.0|12.0|200.0|85.0|-0.5|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   1| 3.9|Nongzhanguan| 8.0| 6.0|14.0|200.0|84.0|-0.7|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   2| 5.3|Nongzhanguan| 3.0| 5.0|14.0|200.0|83.0|-1.2|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   3| 4.9|Nongzhanguan| 5.0| 5.0|14.0|200.0|84.0|-1.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   4| 3.2|Nongzhanguan| 5.0| 6.0|21.0|200.0|77.0|-1.9|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   5| 2.4|Nongzhanguan| 3.0|13.0|21.0|300.0|77.0|-2.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   6| 2.2|Nongzhanguan| 4.0|15.0|32.0|300.0|62.0|-2.5|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   7| 3.0|Nongzhanguan| 3.0|14.0|45.0|400.0|48.0|-1.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   8| 4.6|Nongzhanguan| 3.0|13.0|49.0|400.0|46.0|-0.3|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   9| 5.5|Nongzhanguan|11.0| 9.0|28.0|400.0|68.0| 0.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  10| 5.2|Nongzhanguan| 3.0| 6.0|20.0|300.0|77.0| 1.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  11| 5.3|Nongzhanguan| 3.0| 6.0|19.0|300.0|81.0| 2.9|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  12| 4.6|Nongzhanguan| 3.0| 4.0|17.0|300.0|86.0| 4.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  13| 4.5|Nongzhanguan| 3.0| 4.0|17.0|300.0|87.0| 5.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  14| 4.0|Nongzhanguan| 6.0| 4.0|19.0|300.0|86.0| 6.2|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  15| 2.1|Nongzhanguan| 5.0| 4.0|18.0|300.0|89.0| 6.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  16| 1.8|Nongzhanguan| 5.0| 4.0|21.0|300.0|85.0| 5.6|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  17| 3.5|Nongzhanguan|10.0| 8.0|25.0|400.0|80.0| 4.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  18| 0.5|Nongzhanguan|11.0| 7.0|27.0|400.0|77.0| 3.2|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  19| 1.1|Nongzhanguan|11.0|12.0|39.0|400.0|64.0| 3.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer, VectorAssembler, StandardScaler, StringIndexer\n",
    "cat_column = [\"station\",\"Season\",\"PM_Rating\"]\n",
    "for i in cat_column:\n",
    "    indexer = StringIndexer(inputCol=i, outputCol=i + \"Index\")\n",
    "    model = indexer.fit(df)\n",
    "    new = model.transform(df)\n",
    "    df = new\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- WSPM: float (nullable = true)\n",
      " |-- PM25: float (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      " |-- O3: float (nullable = true)\n",
      " |-- TEMP: float (nullable = true)\n",
      " |-- stationIndex: double (nullable = true)\n",
      " |-- SeasonIndex: double (nullable = true)\n",
      " |-- PM_RatingIndex: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "new_df = df.drop(*[\"station\",\"Season\",\"PM_Rating\"])\n",
    "new_df = new_df.withColumn(\"year\", new_df[\"year\"].cast(\"integer\")).withColumn(\"month\", new_df[\"month\"].cast(\"integer\"))\\\n",
    ".withColumn(\"hour\", new_df[\"hour\"].cast(\"integer\"))\\\n",
    ".withColumn(\"day\", new_df[\"day\"].cast(\"integer\"))\\\n",
    ".withColumn(\"PM25\", new_df[\"PM25\"].cast(\"float\"))\\\n",
    ".withColumn(\"SO2\", new_df[\"SO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"NO2\", new_df[\"NO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"CO\", new_df[\"CO\"].cast(\"float\"))\\\n",
    ".withColumn(\"O3\", new_df[\"O3\"].cast(\"float\"))\\\n",
    ".withColumn(\"TEMP\", new_df[\"TEMP\"].cast(\"float\"))\\\n",
    ".withColumn(\"WSPM\", new_df[\"WSPM\"].cast(\"float\"))\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[2013.0,3.0,1.0,0...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,2...|  2.0|\n",
      "|[2013.0,3.0,1.0,3...|  2.0|\n",
      "|[2013.0,3.0,1.0,4...|  2.0|\n",
      "|[2013.0,3.0,1.0,5...|  2.0|\n",
      "|[2013.0,3.0,1.0,6...|  2.0|\n",
      "|[2013.0,3.0,1.0,7...|  2.0|\n",
      "|[2013.0,3.0,1.0,8...|  2.0|\n",
      "|[2013.0,3.0,1.0,9...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "f = RFormula(formula=\"PM_RatingIndex~.\",featuresCol=\"features\",labelCol=\"label\")\n",
    "output = f.fit(new_df).transform(new_df)\n",
    "feature_selection = output.select(\"features\",\"label\")\n",
    "feature_selection.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=int(4), maxDepth=int(4), labelCol='label', seed=11)\n",
    "model = rf.fit(feature_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSPM 0.0031046575313197823\n",
      "NO2 0.04105564595078393\n",
      "stationIndex 0.0\n",
      "CO 0.15674951788396643\n",
      "O3 0.005348908852294339\n",
      "PM25 0.7251600464051746\n",
      "year 0.0\n",
      "SO2 0.0470898877636736\n",
      "day 0.0007838588905914809\n",
      "hour 0.00025234408880426407\n",
      "SeasonIndex 0.013331504371405078\n",
      "TEMP 0.007123628261986507\n",
      "month 0.0\n"
     ]
    }
   ],
   "source": [
    "new_model=model.featureImportances \n",
    "importantAttributes = [float(c) for c in new_model]\n",
    "colList = new_df.columns\n",
    "value_result = dict(zip(colList,importantAttributes))\n",
    "for i in value_result:\n",
    "    print(i +\" \" + str(value_result[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
