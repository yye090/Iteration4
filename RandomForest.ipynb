{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/ubuntu/spark-2.1.1-bin-hadoop2.7\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"yye090_722\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"./final1.csv\",format =\"csv\",header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- WSPM: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- PM25: string (nullable = true)\n",
      " |-- SO2: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- CO: string (nullable = true)\n",
      " |-- O3: string (nullable = true)\n",
      " |-- TEMP: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- PM_Rating: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- WSPM: float (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- PM25: string (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      " |-- O3: float (nullable = true)\n",
      " |-- TEMP: float (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- PM_Rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"year\", df[\"year\"].cast(\"integer\")).withColumn(\"month\", df[\"month\"].cast(\"integer\"))\\\n",
    ".withColumn(\"hour\", df[\"hour\"].cast(\"integer\"))\\\n",
    ".withColumn(\"day\", df[\"day\"].cast(\"integer\"))\\\n",
    ".withColumn(\"SO2\", df[\"SO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"NO2\", df[\"NO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"CO\", df[\"CO\"].cast(\"float\"))\\\n",
    ".withColumn(\"O3\", df[\"O3\"].cast(\"float\"))\\\n",
    ".withColumn(\"TEMP\", df[\"TEMP\"].cast(\"float\"))\\\n",
    ".withColumn(\"WSPM\", df[\"WSPM\"].cast(\"float\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+------------+-----------+--------------+\n",
      "|year|month|day|hour|WSPM|     station|PM25| SO2| NO2|   CO|  O3|TEMP|Season|PM_Rating|stationIndex|SeasonIndex|PM_RatingIndex|\n",
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+------------+-----------+--------------+\n",
      "|2013|    3|  1|   0| 5.7|Nongzhanguan| 5.0| 4.0|12.0|200.0|85.0|-0.5|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   1| 3.9|Nongzhanguan| 8.0| 6.0|14.0|200.0|84.0|-0.7|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   2| 5.3|Nongzhanguan| 3.0| 5.0|14.0|200.0|83.0|-1.2|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   3| 4.9|Nongzhanguan| 5.0| 5.0|14.0|200.0|84.0|-1.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   4| 3.2|Nongzhanguan| 5.0| 6.0|21.0|200.0|77.0|-1.9|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   5| 2.4|Nongzhanguan| 3.0|13.0|21.0|300.0|77.0|-2.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   6| 2.2|Nongzhanguan| 4.0|15.0|32.0|300.0|62.0|-2.5|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   7| 3.0|Nongzhanguan| 3.0|14.0|45.0|400.0|48.0|-1.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   8| 4.6|Nongzhanguan| 3.0|13.0|49.0|400.0|46.0|-0.3|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   9| 5.5|Nongzhanguan|11.0| 9.0|28.0|400.0|68.0| 0.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  10| 5.2|Nongzhanguan| 3.0| 6.0|20.0|300.0|77.0| 1.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  11| 5.3|Nongzhanguan| 3.0| 6.0|19.0|300.0|81.0| 2.9|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  12| 4.6|Nongzhanguan| 3.0| 4.0|17.0|300.0|86.0| 4.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  13| 4.5|Nongzhanguan| 3.0| 4.0|17.0|300.0|87.0| 5.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  14| 4.0|Nongzhanguan| 6.0| 4.0|19.0|300.0|86.0| 6.2|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  15| 2.1|Nongzhanguan| 5.0| 4.0|18.0|300.0|89.0| 6.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  16| 1.8|Nongzhanguan| 5.0| 4.0|21.0|300.0|85.0| 5.6|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  17| 3.5|Nongzhanguan|10.0| 8.0|25.0|400.0|80.0| 4.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  18| 0.5|Nongzhanguan|11.0| 7.0|27.0|400.0|77.0| 3.2|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  19| 1.1|Nongzhanguan|11.0|12.0|39.0|400.0|64.0| 3.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer, VectorAssembler, StandardScaler, StringIndexer\n",
    "cat_column = [\"station\",\"Season\",\"PM_Rating\"]\n",
    "for i in cat_column:\n",
    "    indexer = StringIndexer(inputCol=i, outputCol=i + \"Index\")\n",
    "    model = indexer.fit(df)\n",
    "    new = model.transform(df)\n",
    "    df = new\n",
    "df.show()\n",
    "df = df.drop(*[\"PM25\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- WSPM: float (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      " |-- O3: float (nullable = true)\n",
      " |-- TEMP: float (nullable = true)\n",
      " |-- stationIndex: double (nullable = true)\n",
      " |-- SeasonIndex: double (nullable = true)\n",
      " |-- PM_RatingIndex: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "new_df = df.drop(*[\"station\",\"Season\",\"PM_Rating\"])\n",
    "new_df = new_df.withColumn(\"year\", new_df[\"year\"].cast(\"integer\")).withColumn(\"month\", new_df[\"month\"].cast(\"integer\"))\\\n",
    ".withColumn(\"hour\", new_df[\"hour\"].cast(\"integer\"))\\\n",
    ".withColumn(\"day\", new_df[\"day\"].cast(\"integer\"))\\\n",
    ".withColumn(\"SO2\", new_df[\"SO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"NO2\", new_df[\"NO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"CO\", new_df[\"CO\"].cast(\"float\"))\\\n",
    ".withColumn(\"O3\", new_df[\"O3\"].cast(\"float\"))\\\n",
    ".withColumn(\"TEMP\", new_df[\"TEMP\"].cast(\"float\"))\\\n",
    ".withColumn(\"WSPM\", new_df[\"WSPM\"].cast(\"float\"))\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[2013.0,3.0,1.0,0...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,2...|  2.0|\n",
      "|[2013.0,3.0,1.0,3...|  2.0|\n",
      "|[2013.0,3.0,1.0,4...|  2.0|\n",
      "|[2013.0,3.0,1.0,5...|  2.0|\n",
      "|[2013.0,3.0,1.0,6...|  2.0|\n",
      "|[2013.0,3.0,1.0,7...|  2.0|\n",
      "|[2013.0,3.0,1.0,8...|  2.0|\n",
      "|[2013.0,3.0,1.0,9...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "f = RFormula(formula=\"PM_RatingIndex~.\",featuresCol=\"features\",labelCol=\"label\")\n",
    "output = f.fit(new_df).transform(new_df)\n",
    "feature_selection = output.select(\"features\",\"label\")\n",
    "feature_selection.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "(trainingData, testData) = feature_selection.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(numTrees = 3,maxDepth = 5,seed=42,labelCol=\"label\",featuresCol=\"features\")\n",
    "model = model.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction: 36714 total data: 64306 accuracy: 0.5709265076353683\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(testData)\n",
    "predfinal = prediction.select(\"features\",\"prediction\",\"probability\",\"label\")\n",
    "correcPrediction = predfinal.filter(predfinal.prediction ==predfinal.label).count()\n",
    "totaldata = predfinal.count()\n",
    "print(\"Correct prediction: \" + str(correcPrediction) + \" total data: \" + str(totaldata) + \" accuracy: \" + \\\n",
    "      str(correcPrediction/totaldata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC\n",
      "0.8223484117239803\n"
     ]
    }
   ],
   "source": [
    "print(\"RFC\")\n",
    "print(my_binary_eval.evaluate(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random forest ensemble has an accuracy of: 57.09%\n",
      "Test Error = 0.429073\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rfc_acc = acc_evaluator.evaluate(prediction)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print(\"Test Error = %g\" % (1.0 - rfc_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
