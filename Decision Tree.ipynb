{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/ubuntu/spark-2.1.1-bin-hadoop2.7\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"yye090_722\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"./final1.csv\",format =\"csv\",header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- WSPM: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- PM25: string (nullable = true)\n",
      " |-- SO2: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- CO: string (nullable = true)\n",
      " |-- O3: string (nullable = true)\n",
      " |-- TEMP: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- PM_Rating: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- WSPM: float (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- PM25: string (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      " |-- O3: float (nullable = true)\n",
      " |-- TEMP: float (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- PM_Rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"year\", df[\"year\"].cast(\"integer\")).withColumn(\"month\", df[\"month\"].cast(\"integer\"))\\\n",
    ".withColumn(\"hour\", df[\"hour\"].cast(\"integer\"))\\\n",
    ".withColumn(\"day\", df[\"day\"].cast(\"integer\"))\\\n",
    ".withColumn(\"SO2\", df[\"SO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"NO2\", df[\"NO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"CO\", df[\"CO\"].cast(\"float\"))\\\n",
    ".withColumn(\"O3\", df[\"O3\"].cast(\"float\"))\\\n",
    ".withColumn(\"TEMP\", df[\"TEMP\"].cast(\"float\"))\\\n",
    ".withColumn(\"WSPM\", df[\"WSPM\"].cast(\"float\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+------------+-----------+--------------+\n",
      "|year|month|day|hour|WSPM|     station|PM25| SO2| NO2|   CO|  O3|TEMP|Season|PM_Rating|stationIndex|SeasonIndex|PM_RatingIndex|\n",
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+------------+-----------+--------------+\n",
      "|2013|    3|  1|   0| 5.7|Nongzhanguan| 5.0| 4.0|12.0|200.0|85.0|-0.5|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   1| 3.9|Nongzhanguan| 8.0| 6.0|14.0|200.0|84.0|-0.7|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   2| 5.3|Nongzhanguan| 3.0| 5.0|14.0|200.0|83.0|-1.2|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   3| 4.9|Nongzhanguan| 5.0| 5.0|14.0|200.0|84.0|-1.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   4| 3.2|Nongzhanguan| 5.0| 6.0|21.0|200.0|77.0|-1.9|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   5| 2.4|Nongzhanguan| 3.0|13.0|21.0|300.0|77.0|-2.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   6| 2.2|Nongzhanguan| 4.0|15.0|32.0|300.0|62.0|-2.5|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   7| 3.0|Nongzhanguan| 3.0|14.0|45.0|400.0|48.0|-1.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   8| 4.6|Nongzhanguan| 3.0|13.0|49.0|400.0|46.0|-0.3|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|   9| 5.5|Nongzhanguan|11.0| 9.0|28.0|400.0|68.0| 0.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  10| 5.2|Nongzhanguan| 3.0| 6.0|20.0|300.0|77.0| 1.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  11| 5.3|Nongzhanguan| 3.0| 6.0|19.0|300.0|81.0| 2.9|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  12| 4.6|Nongzhanguan| 3.0| 4.0|17.0|300.0|86.0| 4.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  13| 4.5|Nongzhanguan| 3.0| 4.0|17.0|300.0|87.0| 5.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  14| 4.0|Nongzhanguan| 6.0| 4.0|19.0|300.0|86.0| 6.2|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  15| 2.1|Nongzhanguan| 5.0| 4.0|18.0|300.0|89.0| 6.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  16| 1.8|Nongzhanguan| 5.0| 4.0|21.0|300.0|85.0| 5.6|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  17| 3.5|Nongzhanguan|10.0| 8.0|25.0|400.0|80.0| 4.4|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  18| 0.5|Nongzhanguan|11.0| 7.0|27.0|400.0|77.0| 3.2|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "|2013|    3|  1|  19| 1.1|Nongzhanguan|11.0|12.0|39.0|400.0|64.0| 3.0|Spring|Excellent|         7.0|        2.0|           2.0|\n",
      "+----+-----+---+----+----+------------+----+----+----+-----+----+----+------+---------+------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer, VectorAssembler, StandardScaler, StringIndexer\n",
    "cat_column = [\"station\",\"Season\",\"PM_Rating\"]\n",
    "for i in cat_column:\n",
    "    indexer = StringIndexer(inputCol=i, outputCol=i + \"Index\")\n",
    "    model = indexer.fit(df)\n",
    "    new = model.transform(df)\n",
    "    df = new\n",
    "df.show()\n",
    "df = df.drop(*[\"PM25\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- WSPM: float (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      " |-- O3: float (nullable = true)\n",
      " |-- TEMP: float (nullable = true)\n",
      " |-- stationIndex: double (nullable = true)\n",
      " |-- SeasonIndex: double (nullable = true)\n",
      " |-- PM_RatingIndex: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "new_df = df.drop(*[\"station\",\"Season\",\"PM_Rating\"])\n",
    "new_df = new_df.withColumn(\"year\", new_df[\"year\"].cast(\"integer\")).withColumn(\"month\", new_df[\"month\"].cast(\"integer\"))\\\n",
    ".withColumn(\"hour\", new_df[\"hour\"].cast(\"integer\"))\\\n",
    ".withColumn(\"day\", new_df[\"day\"].cast(\"integer\"))\\\n",
    ".withColumn(\"SO2\", new_df[\"SO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"NO2\", new_df[\"NO2\"].cast(\"float\"))\\\n",
    ".withColumn(\"CO\", new_df[\"CO\"].cast(\"float\"))\\\n",
    ".withColumn(\"O3\", new_df[\"O3\"].cast(\"float\"))\\\n",
    ".withColumn(\"TEMP\", new_df[\"TEMP\"].cast(\"float\"))\\\n",
    ".withColumn(\"WSPM\", new_df[\"WSPM\"].cast(\"float\"))\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[2013.0,3.0,1.0,0...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,2...|  2.0|\n",
      "|[2013.0,3.0,1.0,3...|  2.0|\n",
      "|[2013.0,3.0,1.0,4...|  2.0|\n",
      "|[2013.0,3.0,1.0,5...|  2.0|\n",
      "|[2013.0,3.0,1.0,6...|  2.0|\n",
      "|[2013.0,3.0,1.0,7...|  2.0|\n",
      "|[2013.0,3.0,1.0,8...|  2.0|\n",
      "|[2013.0,3.0,1.0,9...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "|[2013.0,3.0,1.0,1...|  2.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "f = RFormula(formula=\"PM_RatingIndex~.\",featuresCol=\"features\",labelCol=\"label\")\n",
    "output = f.fit(new_df).transform(new_df)\n",
    "feature_selection = output.select(\"features\",\"label\")\n",
    "feature_selection.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "(trainingData, testData) = feature_selection.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction: 37798 total data: 64696 accuracy: 0.584240138493879\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(testData)\n",
    "predfinal = prediction.select(\"features\",\"prediction\",\"probability\",\"label\")\n",
    "correcPrediction = predfinal.filter(predfinal.prediction ==predfinal.label).count()\n",
    "totaldata = predfinal.count()\n",
    "print(\"Correct prediction: \" + str(correcPrediction) + \" total data: \" + str(totaldata) + \" accuracy: \" +\\\n",
    "      str(correcPrediction/totaldata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC\n",
      "0.731642977034116\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'label')\n",
    "print(\"DTC\")\n",
    "print(my_binary_eval.evaluate(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A decision tree has an accuracy of: 58.42%\n",
      "Test Error = 0.41576\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rfc_acc = acc_evaluator.evaluate(prediction)\n",
    "print('A decision tree has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print(\"Test Error = %g\" % (1.0 - rfc_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_483f9caa32a5d4eb0db5) of depth 5 with 63 nodes\n",
      "  If (feature 7 <= 700.0)\n",
      "   If (feature 7 <= 400.0)\n",
      "    If (feature 11 in {0.0,2.0})\n",
      "     If (feature 9 <= 8.100000381469727)\n",
      "      If (feature 1 <= 3.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 1 > 3.0)\n",
      "       Predict: 2.0\n",
      "     Else (feature 9 > 8.100000381469727)\n",
      "      If (feature 6 <= 15.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 6 > 15.0)\n",
      "       Predict: 1.0\n",
      "    Else (feature 11 not in {0.0,2.0})\n",
      "     If (feature 7 <= 300.0)\n",
      "      If (feature 6 <= 13.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 6 > 13.0)\n",
      "       Predict: 2.0\n",
      "     Else (feature 7 > 300.0)\n",
      "      If (feature 9 <= 2.5999999046325684)\n",
      "       Predict: 2.0\n",
      "      Else (feature 9 > 2.5999999046325684)\n",
      "       Predict: 1.0\n",
      "   Else (feature 7 > 400.0)\n",
      "    If (feature 11 in {2.0})\n",
      "     If (feature 9 <= 14.899999618530273)\n",
      "      If (feature 7 <= 500.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 7 > 500.0)\n",
      "       Predict: 3.0\n",
      "     Else (feature 9 > 14.899999618530273)\n",
      "      If (feature 7 <= 600.0)\n",
      "       Predict: 3.0\n",
      "      Else (feature 7 > 600.0)\n",
      "       Predict: 0.0\n",
      "    Else (feature 11 not in {2.0})\n",
      "     If (feature 9 <= 3.9000000953674316)\n",
      "      If (feature 7 <= 500.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 7 > 500.0)\n",
      "       Predict: 1.0\n",
      "     Else (feature 9 > 3.9000000953674316)\n",
      "      If (feature 7 <= 600.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 7 > 600.0)\n",
      "       Predict: 1.0\n",
      "  Else (feature 7 > 700.0)\n",
      "   If (feature 7 <= 1100.0)\n",
      "    If (feature 9 <= 3.9000000953674316)\n",
      "     If (feature 1 <= 11.0)\n",
      "      If (feature 7 <= 900.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 7 > 900.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 1 > 11.0)\n",
      "      If (feature 7 <= 900.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 7 > 900.0)\n",
      "       Predict: 1.0\n",
      "    Else (feature 9 > 3.9000000953674316)\n",
      "     If (feature 7 <= 800.0)\n",
      "      If (feature 11 in {2.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 11 not in {2.0})\n",
      "       Predict: 0.0\n",
      "     Else (feature 7 > 800.0)\n",
      "      If (feature 11 in {0.0,2.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 11 not in {0.0,2.0})\n",
      "       Predict: 0.0\n",
      "   Else (feature 7 > 1100.0)\n",
      "    If (feature 7 <= 1600.0)\n",
      "     If (feature 9 <= 9.600000381469727)\n",
      "      If (feature 1 <= 11.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 11.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 9 > 9.600000381469727)\n",
      "      If (feature 2 <= 9.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 2 > 9.0)\n",
      "       Predict: 0.0\n",
      "    Else (feature 7 > 1600.0)\n",
      "     If (feature 9 <= 5.300000190734863)\n",
      "      If (feature 7 <= 2300.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 7 > 2300.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 9 > 5.300000190734863)\n",
      "      If (feature 1 <= 9.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 9.0)\n",
      "       Predict: 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'month', 'day', 'hour', 'WSPM', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'stationIndex', 'SeasonIndex', 'PM_RatingIndex']\n"
     ]
    }
   ],
   "source": [
    "print(new_df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
